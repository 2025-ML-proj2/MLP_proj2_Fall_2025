{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":86518,"databundleVersionId":9809560,"sourceType":"competition"},{"sourceId":8897601,"sourceType":"datasetVersion","datasetId":5297895},{"sourceId":9102725,"sourceType":"datasetVersion","datasetId":5493674},{"sourceId":9107824,"sourceType":"datasetVersion","datasetId":5496762},{"sourceId":9107963,"sourceType":"datasetVersion","datasetId":5496847},{"sourceId":9108069,"sourceType":"datasetVersion","datasetId":5496920},{"sourceId":10214229,"sourceType":"datasetVersion","datasetId":6267026},{"sourceId":10302322,"sourceType":"datasetVersion","datasetId":6265823},{"sourceId":75103,"sourceType":"modelInstanceVersion","modelInstanceId":63082,"modelId":86587}],"dockerImageVersionId":30746,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Installing Packages","metadata":{}},{"cell_type":"code","source":"import torch\nprint(torch.cuda.is_available())\nprint(torch.cuda.device_count())\nprint(torch.cuda.get_device_name(0))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T04:58:50.053388Z","iopub.execute_input":"2025-11-06T04:58:50.054087Z","iopub.status.idle":"2025-11-06T04:58:50.063782Z","shell.execute_reply.started":"2025-11-06T04:58:50.054059Z","shell.execute_reply":"2025-11-06T04:58:50.063027Z"}},"outputs":[{"name":"stdout","text":"True\n2\nTesla T4\n","output_type":"stream"}],"execution_count":77},{"cell_type":"code","source":"%pip install /kaggle/input/lmsys-packages/triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T05:03:20.404579Z","iopub.execute_input":"2025-11-06T05:03:20.404940Z","iopub.status.idle":"2025-11-06T05:03:33.231776Z","shell.execute_reply.started":"2025-11-06T05:03:20.404914Z","shell.execute_reply":"2025-11-06T05:03:33.230652Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Processing /kaggle/input/lmsys-packages/triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from triton==2.2.0) (3.13.1)\ntriton is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":79},{"cell_type":"code","source":"%pip install /kaggle/input/lmsys-packages/xformers-0.0.24042abc8.d20240802-cp310-cp310-linux_x86_64.whl","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T05:03:33.241038Z","iopub.execute_input":"2025-11-06T05:03:33.241336Z","iopub.status.idle":"2025-11-06T05:03:45.561120Z","shell.execute_reply.started":"2025-11-06T05:03:33.241315Z","shell.execute_reply":"2025-11-06T05:03:45.560065Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Processing /kaggle/input/lmsys-packages/xformers-0.0.24042abc8.d20240802-cp310-cp310-linux_x86_64.whl\nRequirement already satisfied: torch>=1.12 in /opt/conda/lib/python3.10/site-packages (from xformers==0.0.24042abc8.d20240802) (2.1.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from xformers==0.0.24042abc8.d20240802) (1.26.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.12->xformers==0.0.24042abc8.d20240802) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.12->xformers==0.0.24042abc8.d20240802) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.12->xformers==0.0.24042abc8.d20240802) (1.13.0)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.12->xformers==0.0.24042abc8.d20240802) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.12->xformers==0.0.24042abc8.d20240802) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.12->xformers==0.0.24042abc8.d20240802) (2024.5.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.12->xformers==0.0.24042abc8.d20240802) (2.1.3)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.12->xformers==0.0.24042abc8.d20240802) (1.3.0)\nxformers is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":81},{"cell_type":"code","source":"%pip install /kaggle/input/some-pack/faiss_cpu_downloads/faiss_cpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T05:03:46.796540Z","iopub.execute_input":"2025-11-06T05:03:46.796816Z","iopub.status.idle":"2025-11-06T05:03:55.579944Z","shell.execute_reply.started":"2025-11-06T05:03:46.796792Z","shell.execute_reply":"2025-11-06T05:03:55.578884Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Processing /kaggle/input/some-pack/faiss_cpu_downloads/faiss_cpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\nfaiss-cpu is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":83},{"cell_type":"code","source":"%pip install --no-index --find-links=/kaggle/input/some-pack/sentence_transformers_packages sentence-transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T05:03:55.597419Z","iopub.execute_input":"2025-11-06T05:03:55.597649Z","iopub.status.idle":"2025-11-06T05:04:04.433340Z","shell.execute_reply.started":"2025-11-06T05:03:55.597613Z","shell.execute_reply":"2025-11-06T05:04:04.432084Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Looking in links: /kaggle/input/some-pack/sentence_transformers_packages\nRequirement already satisfied: sentence-transformers in /opt/conda/lib/python3.10/site-packages (3.3.1)\nRequirement already satisfied: transformers<5.0.0,>=4.41.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.42.3)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.66.4)\nRequirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (2.1.2)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.2.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.11.4)\nRequirement already satisfied: huggingface-hub>=0.20.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (0.23.4)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (9.5.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.5.0)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.0)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.2)\nRequirement already satisfied: numpy<2.0,>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2023.12.25)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.3)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.19.1)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.2.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.20.0->sentence-transformers) (3.1.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.7.4)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":85},{"cell_type":"code","source":"!cp -r /kaggle/input/lmsys-modules-0805 human_pref","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T05:04:04.607030Z","iopub.execute_input":"2025-11-06T05:04:04.607290Z","iopub.status.idle":"2025-11-06T05:04:05.746283Z","shell.execute_reply.started":"2025-11-06T05:04:04.607267Z","shell.execute_reply":"2025-11-06T05:04:05.745317Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}],"execution_count":87},{"cell_type":"markdown","source":"## Check input files","metadata":{}},{"cell_type":"code","source":"import os\n\n# Walk through all directories and files under the '/kaggle/input' folder\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    # Loop through each file in the current directory\n    for filename in filenames:\n        # Print the full path to the file by joining the directory and filename\n        print(os.path.join(dirname, filename))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T05:04:10.505505Z","iopub.execute_input":"2025-11-06T05:04:10.505851Z","iopub.status.idle":"2025-11-06T05:04:10.614348Z","shell.execute_reply.started":"2025-11-06T05:04:10.505809Z","shell.execute_reply":"2025-11-06T05:04:10.613491Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/lmsys-packages/triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n/kaggle/input/lmsys-packages/xformers-0.0.24042abc8.d20240802-cp310-cp310-linux_x86_64.whl\n/kaggle/input/gemma-2/transformers/gemma-2-9b-it-4bit/1/gemma-2-9b-it-4bit/model.safetensors.index.json\n/kaggle/input/gemma-2/transformers/gemma-2-9b-it-4bit/1/gemma-2-9b-it-4bit/config.json\n/kaggle/input/gemma-2/transformers/gemma-2-9b-it-4bit/1/gemma-2-9b-it-4bit/model-00001-of-00002.safetensors\n/kaggle/input/gemma-2/transformers/gemma-2-9b-it-4bit/1/gemma-2-9b-it-4bit/model-00002-of-00002.safetensors\n/kaggle/input/gemma-2/transformers/gemma-2-9b-it-4bit/1/gemma-2-9b-it-4bit/tokenizer.json\n/kaggle/input/gemma-2/transformers/gemma-2-9b-it-4bit/1/gemma-2-9b-it-4bit/tokenizer_config.json\n/kaggle/input/gemma-2/transformers/gemma-2-9b-it-4bit/1/gemma-2-9b-it-4bit/special_tokens_map.json\n/kaggle/input/gemma-2/transformers/gemma-2-9b-it-4bit/1/gemma-2-9b-it-4bit/tokenizer.model\n/kaggle/input/some-pack/balanced_transformed_dataset.csv\n/kaggle/input/some-pack/faiss_cpu_downloads/faiss_cpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n/kaggle/input/some-pack/sentence-transformer-model/config.json\n/kaggle/input/some-pack/sentence-transformer-model/README.md\n/kaggle/input/some-pack/sentence-transformer-model/tokenizer.json\n/kaggle/input/some-pack/sentence-transformer-model/tokenizer_config.json\n/kaggle/input/some-pack/sentence-transformer-model/sentence_bert_config.json\n/kaggle/input/some-pack/sentence-transformer-model/config_sentence_transformers.json\n/kaggle/input/some-pack/sentence-transformer-model/model.safetensors\n/kaggle/input/some-pack/sentence-transformer-model/modules.json\n/kaggle/input/some-pack/sentence-transformer-model/special_tokens_map.json\n/kaggle/input/some-pack/sentence-transformer-model/vocab.txt\n/kaggle/input/some-pack/sentence-transformer-model/1_Pooling/config.json\n/kaggle/input/some-pack/sentence_transformers_packages/MarkupSafe-3.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n/kaggle/input/some-pack/sentence_transformers_packages/nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl\n/kaggle/input/some-pack/sentence_transformers_packages/scipy-1.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n/kaggle/input/some-pack/sentence_transformers_packages/pillow-11.0.0-cp310-cp310-manylinux_2_28_x86_64.whl\n/kaggle/input/some-pack/sentence_transformers_packages/regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n/kaggle/input/some-pack/sentence_transformers_packages/nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl\n/kaggle/input/some-pack/sentence_transformers_packages/certifi-2024.12.14-py3-none-any.whl\n/kaggle/input/some-pack/sentence_transformers_packages/nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl\n/kaggle/input/some-pack/sentence_transformers_packages/nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl\n/kaggle/input/some-pack/sentence_transformers_packages/nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl\n/kaggle/input/some-pack/sentence_transformers_packages/tqdm-4.67.1-py3-none-any.whl\n/kaggle/input/some-pack/sentence_transformers_packages/sentence_transformers-3.3.1-py3-none-any.whl\n/kaggle/input/some-pack/sentence_transformers_packages/PyYAML-6.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n/kaggle/input/some-pack/sentence_transformers_packages/triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n/kaggle/input/some-pack/sentence_transformers_packages/jinja2-3.1.4-py3-none-any.whl\n/kaggle/input/some-pack/sentence_transformers_packages/scikit_learn-1.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n/kaggle/input/some-pack/sentence_transformers_packages/tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n/kaggle/input/some-pack/sentence_transformers_packages/nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl\n/kaggle/input/some-pack/sentence_transformers_packages/idna-3.10-py3-none-any.whl\n/kaggle/input/some-pack/sentence_transformers_packages/mpmath-1.3.0-py3-none-any.whl\n/kaggle/input/some-pack/sentence_transformers_packages/fsspec-2024.10.0-py3-none-any.whl\n/kaggle/input/some-pack/sentence_transformers_packages/safetensors-0.4.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n/kaggle/input/some-pack/sentence_transformers_packages/networkx-3.4.2-py3-none-any.whl\n/kaggle/input/some-pack/sentence_transformers_packages/transformers-4.47.0-py3-none-any.whl\n/kaggle/input/some-pack/sentence_transformers_packages/filelock-3.16.1-py3-none-any.whl\n/kaggle/input/some-pack/sentence_transformers_packages/threadpoolctl-3.5.0-py3-none-any.whl\n/kaggle/input/some-pack/sentence_transformers_packages/nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl\n/kaggle/input/some-pack/sentence_transformers_packages/numpy-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n/kaggle/input/some-pack/sentence_transformers_packages/nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl\n/kaggle/input/some-pack/sentence_transformers_packages/nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl\n/kaggle/input/some-pack/sentence_transformers_packages/requests-2.32.3-py3-none-any.whl\n/kaggle/input/some-pack/sentence_transformers_packages/nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl\n/kaggle/input/some-pack/sentence_transformers_packages/nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl\n/kaggle/input/some-pack/sentence_transformers_packages/joblib-1.4.2-py3-none-any.whl\n/kaggle/input/some-pack/sentence_transformers_packages/urllib3-2.2.3-py3-none-any.whl\n/kaggle/input/some-pack/sentence_transformers_packages/typing_extensions-4.12.2-py3-none-any.whl\n/kaggle/input/some-pack/sentence_transformers_packages/sympy-1.13.1-py3-none-any.whl\n/kaggle/input/some-pack/sentence_transformers_packages/huggingface_hub-0.26.5-py3-none-any.whl\n/kaggle/input/some-pack/sentence_transformers_packages/torch-2.5.1-cp310-cp310-manylinux1_x86_64.whl\n/kaggle/input/some-pack/sentence_transformers_packages/packaging-24.2-py3-none-any.whl\n/kaggle/input/some-pack/sentence_transformers_packages/nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl\n/kaggle/input/some-pack/sentence_transformers_packages/charset_normalizer-3.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n/kaggle/input/llm-classification-finetuning/sample_submission.csv\n/kaggle/input/llm-classification-finetuning/train.csv\n/kaggle/input/llm-classification-finetuning/test.csv\n/kaggle/input/lmsys-checkpoints-3-0805/model.safetensors.index.json\n/kaggle/input/lmsys-checkpoints-3-0805/model-00003-of-00004.safetensors\n/kaggle/input/lmsys-checkpoints-3-0805/config.json\n/kaggle/input/lmsys-checkpoints-3-0805/model-00001-of-00004.safetensors\n/kaggle/input/lmsys-checkpoints-3-0805/tokenizer.json\n/kaggle/input/lmsys-checkpoints-3-0805/tokenizer_config.json\n/kaggle/input/lmsys-checkpoints-3-0805/model-00004-of-00004.safetensors\n/kaggle/input/lmsys-checkpoints-3-0805/special_tokens_map.json\n/kaggle/input/lmsys-checkpoints-3-0805/model-00002-of-00004.safetensors\n/kaggle/input/lmsys-modules-0805/utils.py\n/kaggle/input/lmsys-modules-0805/models/modeling_gemma2.py\n/kaggle/input/lmsys-modules-0805/models/modeling_llama.py\n/kaggle/input/lmsys-modules-0805/models/ops/triton_utils.py\n/kaggle/input/lmsys-modules-0805/models/ops/rms_norm.py\n/kaggle/input/lmsys-modules-0805/models/ops/flash_attention_nopad.py\n/kaggle/input/lmsys-modules-0805/models/ops/fused_rotary_emb.py\n/kaggle/input/lmsys-modules-0805/models/ops/gelu_and_mul.py\n/kaggle/input/lmsys-modules-0805/models/ops/silu_and_mul.py\n/kaggle/input/lmsys-modules-0805/data/collators.py\n/kaggle/input/lmsys-modules-0805/data/dataset.py\n/kaggle/input/lmsys-modules-0805/data/processors.py\n/kaggle/input/akemiiiiii/balanced_transformed_dataset.csv\n/kaggle/input/akemiiiiii/custom_model_dir/spm.model\n/kaggle/input/akemiiiiii/custom_model_dir/custom_model_complete.pth\n/kaggle/input/akemiiiiii/custom_model_dir/tokenizer.json\n/kaggle/input/akemiiiiii/custom_model_dir/tokenizer_config.json\n/kaggle/input/akemiiiiii/custom_model_dir/model_config.pth\n/kaggle/input/akemiiiiii/custom_model_dir/special_tokens_map.json\n/kaggle/input/akemiiiiii/custom_model_dir/custom_model_weights.pth\n/kaggle/input/akemiiiiii/custom_model_dir/added_tokens.json\n/kaggle/input/lmsys-wheel-files/peft-0.11.1-py3-none-any.whl\n/kaggle/input/lmsys-wheel-files/transformers-4.42.3-py3-none-any.whl\n/kaggle/input/lmsys-wheel-files/sympy-1.12.1-py3-none-any.whl\n/kaggle/input/lmsys-wheel-files/numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n/kaggle/input/lmsys-wheel-files/urllib3-2.2.2-py3-none-any.whl\n/kaggle/input/lmsys-wheel-files/triton-2.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n/kaggle/input/lmsys-wheel-files/tqdm-4.66.4-py3-none-any.whl\n/kaggle/input/lmsys-wheel-files/nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl\n/kaggle/input/lmsys-wheel-files/idna-3.7-py3-none-any.whl\n/kaggle/input/lmsys-wheel-files/nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl\n/kaggle/input/lmsys-wheel-files/nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl\n/kaggle/input/lmsys-wheel-files/jinja2-3.1.4-py3-none-any.whl\n/kaggle/input/lmsys-wheel-files/nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl\n/kaggle/input/lmsys-wheel-files/mpmath-1.3.0-py3-none-any.whl\n/kaggle/input/lmsys-wheel-files/networkx-3.3-py3-none-any.whl\n/kaggle/input/lmsys-wheel-files/torch-2.3.1-cp310-cp310-manylinux1_x86_64.whl\n/kaggle/input/lmsys-wheel-files/psutil-6.0.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n/kaggle/input/lmsys-wheel-files/filelock-3.15.4-py3-none-any.whl\n/kaggle/input/lmsys-wheel-files/regex-2024.5.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n/kaggle/input/lmsys-wheel-files/PyYAML-6.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n/kaggle/input/lmsys-wheel-files/certifi-2024.7.4-py3-none-any.whl\n/kaggle/input/lmsys-wheel-files/MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n/kaggle/input/lmsys-wheel-files/accelerate-0.32.1-py3-none-any.whl\n/kaggle/input/lmsys-wheel-files/nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl\n/kaggle/input/lmsys-wheel-files/safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n/kaggle/input/lmsys-wheel-files/nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl\n/kaggle/input/lmsys-wheel-files/packaging-24.1-py3-none-any.whl\n/kaggle/input/lmsys-wheel-files/nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl\n/kaggle/input/lmsys-wheel-files/nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl\n/kaggle/input/lmsys-wheel-files/bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl\n/kaggle/input/lmsys-wheel-files/nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl\n/kaggle/input/lmsys-wheel-files/requests-2.32.3-py3-none-any.whl\n/kaggle/input/lmsys-wheel-files/charset_normalizer-3.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n/kaggle/input/lmsys-wheel-files/typing_extensions-4.12.2-py3-none-any.whl\n/kaggle/input/lmsys-wheel-files/huggingface_hub-0.23.4-py3-none-any.whl\n/kaggle/input/lmsys-wheel-files/nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl\n/kaggle/input/lmsys-wheel-files/fsspec-2024.6.1-py3-none-any.whl\n/kaggle/input/lmsys-wheel-files/nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl\n/kaggle/input/lmsys-wheel-files/nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl\n/kaggle/input/lmsys-wheel-files/tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n/kaggle/input/lmsys-checkpoints-0-0805/model.safetensors.index.json\n/kaggle/input/lmsys-checkpoints-0-0805/model-00003-of-00004.safetensors\n/kaggle/input/lmsys-checkpoints-0-0805/config.json\n/kaggle/input/lmsys-checkpoints-0-0805/model-00001-of-00004.safetensors\n/kaggle/input/lmsys-checkpoints-0-0805/tokenizer.json\n/kaggle/input/lmsys-checkpoints-0-0805/tokenizer_config.json\n/kaggle/input/lmsys-checkpoints-0-0805/model-00004-of-00004.safetensors\n/kaggle/input/lmsys-checkpoints-0-0805/special_tokens_map.json\n/kaggle/input/lmsys-checkpoints-0-0805/model-00002-of-00004.safetensors\n/kaggle/input/lmsys-checkpoints-0-0805/tokenizer.model\n","output_type":"stream"}],"execution_count":89},{"cell_type":"markdown","source":"# Prepare test file","metadata":{}},{"cell_type":"code","source":"%%writefile test.py\nimport pandas as pd\n\n# Load the original test CSV file from the Kaggle input directory\ndf = pd.read_csv(\"/kaggle/input/llm-classification-finetuning/test.csv\")\n\n# Add dummy label columns indicating model A is always the winner\ndf[\"winner_model_a\"] = 1\ndf[\"winner_model_b\"] = 0\ndf[\"winner_tie\"] = 0\n\n# Save the original test dataframe (with dummy labels) to a Parquet file\ndf.to_parquet(\"test.parquet\", index=False)\n\n# Swap responses A and B — simulating the reverse comparison\ndf[\"response_a\"], df[\"response_b\"] = df[\"response_b\"], df[\"response_a\"]\n\n# Save the swapped version to a separate Parquet file\ndf.to_parquet(\"test_swap.parquet\", index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T05:04:10.625148Z","iopub.execute_input":"2025-11-06T05:04:10.625456Z","iopub.status.idle":"2025-11-06T05:04:10.639773Z","shell.execute_reply.started":"2025-11-06T05:04:10.625427Z","shell.execute_reply":"2025-11-06T05:04:10.638727Z"}},"outputs":[{"name":"stdout","text":"Overwriting test.py\n","output_type":"stream"}],"execution_count":91},{"cell_type":"code","source":"!python test.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T05:04:10.789913Z","iopub.execute_input":"2025-11-06T05:04:10.790241Z","iopub.status.idle":"2025-11-06T05:04:12.498856Z","shell.execute_reply.started":"2025-11-06T05:04:10.790209Z","shell.execute_reply":"2025-11-06T05:04:12.497876Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}],"execution_count":93},{"cell_type":"markdown","source":"# Inference: gemma2-9b","metadata":{}},{"cell_type":"code","source":"%%writefile gemma.py\nimport torch\nimport numpy as np\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nfrom transformers import AutoTokenizer\n\nfrom xformers.ops.fmha.attn_bias import BlockDiagonalCausalMask\nfrom human_pref.models.modeling_gemma2 import Gemma2ForSequenceClassification\nfrom human_pref.data.processors import ProcessorPAB\nfrom human_pref.data.dataset import LMSYSDataset\nfrom human_pref.data.collators import VarlenCollator, ShardedMaxTokensCollator\nfrom human_pref.utils import to_device\n\n# --- Configuration ---\nmodel_name_or_path = \"/kaggle/input/lmsys-checkpoints-0-0805\"  # Pretrained model checkpoint path\ncsv_path = \"test.parquet\"  # Path to input data in .parquet format\n\n# --- Load tokenizer and processor ---\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n\n# Processor to tokenize input samples (prompt-response pairs)\nprocessor = ProcessorPAB(\n    tokenizer=tokenizer,\n    max_length=4608,\n    support_system_role=False,\n)\n\n# Load dataset and apply preprocessing\ndataset = LMSYSDataset(\n    csv_file=csv_path,\n    query=None,\n    processor=processor,\n    include_swap=False,\n    is_parquet=True,\n)\n\n# DataLoader with custom collator to batch examples based on total token count\ndataloader = DataLoader(\n    dataset,\n    batch_size=80,  # Each \"batch\" is a list of micro-batches\n    num_workers=4,\n    collate_fn=ShardedMaxTokensCollator(\n        max_tokens=8192,  # Maximum total tokens per batch\n        base_collator=VarlenCollator()\n    ),\n)\n\n# --- Define pipeline parallelism across 2 GPUs ---\n\n# Total number of transformer layers in the model\nnum_hidden_layers = 42\n\n# Assign embedding, final layers, and score head to GPU 0 and 1\ndevice_map = {\n    \"model.embed_tokens\": \"cuda:0\",\n    \"model.norm\": \"cuda:1\",\n    \"score\": \"cuda:1\",\n}\n\n# Split model layers between two GPUs: first half on cuda:0, second half on cuda:1\nfor i in range(num_hidden_layers // 2):\n    device_map[f\"model.layers.{i}\"] = \"cuda:0\"\nfor i in range(num_hidden_layers // 2, num_hidden_layers):\n    device_map[f\"model.layers.{i}\"] = \"cuda:1\"\n\n# Load model with weights onto corresponding devices and use float16\nmodel = Gemma2ForSequenceClassification.from_pretrained(\n    model_name_or_path,\n    torch_dtype=torch.float16,\n    device_map=device_map,\n)\n\n# --- Prepare rotary embeddings ---\nconfig = model.config\ndim = config.head_dim  # dimension of attention heads\n# Compute inverse frequencies for RoPE (rotary positional encoding)\ninv_freq = 1.0 / (\n    config.rope_theta ** (torch.arange(0, dim, 2, dtype=torch.float32) / dim)\n)\ninv_freq0 = inv_freq.to(\"cuda:0\")  # For first half of model\ninv_freq1 = inv_freq.to(\"cuda:1\")  # For second half\n\n# --- Inference loop using pipelined execution ---\nis_first = True\nhidden_states = None\nouts = []\n\n# Loop through all batches\nfor batch in tqdm(dataloader):\n    # Each batch is a list of micro-batches\n    for micro_batch in batch:\n        # Move input tokens to GPU 0\n        input_ids = to_device(micro_batch[\"input_ids\"], \"cuda:0\")\n\n        # Prepare sequence-related information\n        seq_info = dict(\n            cu_seqlens=micro_batch[\"cu_seqlens\"],\n            position_ids=micro_batch[\"position_ids\"],\n            max_seq_len=micro_batch[\"max_seq_len\"],\n            attn_bias=BlockDiagonalCausalMask.from_seqlens(micro_batch[\"seq_lens\"]),\n        )\n        seq_info = to_device(seq_info, \"cuda:0\")\n\n        # If first iteration, run only part 1 and store state\n        if is_first:\n            with torch.no_grad(), torch.cuda.amp.autocast():\n                prev_hidden_states = model.forward_part1(input_ids, seq_info, inv_freq1)\n            is_first = False\n            # Move intermediate outputs to GPU 1 for next step\n            prev_seq_info, prev_hidden_states = to_device(\n                [seq_info, prev_hidden_states], \"cuda:1\"\n            )\n            continue\n\n        # Run part 2 for previous micro-batch and part 1 for current micro-batch\n        with torch.no_grad(), torch.cuda.amp.autocast():\n            # Compute final logits for previous hidden states\n            logits = model.forward_part2(prev_hidden_states, prev_seq_info, inv_freq1)\n\n            # Compute hidden states for the next micro-batch\n            hidden_states = model.forward_part1(input_ids, seq_info, inv_freq1)\n\n            # Move new hidden state and seq_info to GPU 1\n            prev_seq_info, prev_hidden_states = to_device(\n                [seq_info, hidden_states], \"cuda:1\"\n            )\n            outs.append(logits.cpu())  # Store prediction logits on CPU\n\n# --- Final prediction for the last micro-batch ---\nwith torch.no_grad(), torch.cuda.amp.autocast():\n    logits = model.forward_part2(prev_hidden_states, prev_seq_info, inv_freq1)\n    outs.append(logits.cpu())\n\n# Concatenate all logits and compute probabilities\npred = torch.cat(outs, dim=0)\nprob = pred.softmax(-1)\n\n# Evaluate predictions with dataset's built-in method\nprint(dataset.evaluate(prob.numpy()))\n\n# Save prediction probabilities to file\nnp.save('prob_m0.npy', prob)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T05:04:12.520954Z","iopub.execute_input":"2025-11-06T05:04:12.521242Z","iopub.status.idle":"2025-11-06T05:04:12.528680Z","shell.execute_reply.started":"2025-11-06T05:04:12.521222Z","shell.execute_reply":"2025-11-06T05:04:12.527828Z"}},"outputs":[{"name":"stdout","text":"Overwriting gemma.py\n","output_type":"stream"}],"execution_count":95},{"cell_type":"code","source":"!python gemma.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T05:04:12.530020Z","iopub.execute_input":"2025-11-06T05:04:12.530580Z","iopub.status.idle":"2025-11-06T05:09:45.800630Z","shell.execute_reply.started":"2025-11-06T05:04:12.530551Z","shell.execute_reply":"2025-11-06T05:09:45.799468Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Loading checkpoint shards: 100%|██████████████████| 4/4 [05:12<00:00, 78.07s/it]\n  0%|                                                     | 0/1 [00:00<?, ?it/s]2025-11-06 05:09:32.689420: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2025-11-06 05:09:32.689600: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-11-06 05:09:32.836046: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n100%|█████████████████████████████████████████████| 1/1 [00:11<00:00, 11.07s/it]\n{'log_loss': 3.094615495202658}\n","output_type":"stream"}],"execution_count":96},{"cell_type":"markdown","source":"# Inference: llama3-8b","metadata":{}},{"cell_type":"code","source":"%%writefile llama.py\nimport torch\nimport numpy as np\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nfrom transformers import AutoTokenizer\n\nfrom xformers.ops.fmha.attn_bias import BlockDiagonalCausalMask\nfrom human_pref.models.modeling_llama import LlamaForSequenceClassification\nfrom human_pref.data.processors import ProcessorPAB\nfrom human_pref.data.dataset import LMSYSDataset\nfrom human_pref.data.collators import VarlenCollator, ShardedMaxTokensCollator\nfrom human_pref.utils import to_device\n\n# --- Configurations ---\nmodel_name_or_path = \"/kaggle/input/lmsys-checkpoints-3-0805\"  # Path to pretrained LLaMA checkpoint\ncsv_path = \"test_swap.parquet\"  # Dataset file (responses are swapped)\n\n# --- Tokenizer and Processor setup ---\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n\n# Suppress tokenizer length warning\ntokenizer.deprecation_warnings[\n    \"sequence-length-is-longer-than-the-specified-maximum\"\n] = True\n\n# Processor formats prompt-response input for the model\nprocessor = ProcessorPAB(\n    tokenizer=tokenizer,\n    max_length=4608,\n    support_system_role=True,\n)\n\n# --- Load Dataset ---\ndataset = LMSYSDataset(\n    csv_file=csv_path,\n    query=None,\n    processor=processor,\n    include_swap=False,\n    is_parquet=True,\n)\n\n# --- DataLoader setup with dynamic token batching ---\ndataloader = DataLoader(\n    dataset,\n    batch_size=80,\n    num_workers=4,\n    collate_fn=ShardedMaxTokensCollator(\n        max_tokens=8192,\n        base_collator=VarlenCollator()\n    ),\n)\n\n# --- Device mapping for pipelined parallelism ---\nnum_hidden_layers = 32  # LLaMA-3 has 32 transformer layers\n\ndevice_map = {\n    \"model.embed_tokens\": \"cuda:0\",\n    \"model.norm\": \"cuda:1\",\n    \"score\": \"cuda:1\",\n}\n\n# First half of layers on GPU 0\nfor i in range(num_hidden_layers // 2):\n    device_map[f\"model.layers.{i}\"] = \"cuda:0\"\n\n# Second half of layers on GPU 1\nfor i in range(num_hidden_layers // 2, num_hidden_layers):\n    device_map[f\"model.layers.{i}\"] = \"cuda:1\"\n\n# Load the model with float16 precision using the device map\nmodel = LlamaForSequenceClassification.from_pretrained(\n    model_name_or_path,\n    torch_dtype=torch.float16,\n    device_map=device_map,\n)\n\n# --- Prepare rotary position encodings ---\nconfig = model.config\ndim = config.hidden_size // config.num_attention_heads\n\n# Compute inverse frequencies for rotary embeddings\ninv_freq = 1.0 / (\n    config.rope_theta ** (torch.arange(0, dim, 2, dtype=torch.float32) / dim)\n)\ninv_freq0 = inv_freq.to(\"cuda:0\")\ninv_freq1 = inv_freq.to(\"cuda:1\")\n\n# --- Pipelined Inference across 2 GPUs ---\nis_first = True  # Special case for first micro-batch\nhidden_states = None\nouts = []  # Store output logits\n\n# Loop through batches\nfor batch in tqdm(dataloader):\n    for micro_batch in batch:\n        # Move inputs to GPU 0\n        input_ids = to_device(micro_batch[\"input_ids\"], \"cuda:0\")\n\n        # Construct sequence info for attention mask and positional encoding\n        seq_info = dict(\n            cu_seqlens=micro_batch[\"cu_seqlens\"],\n            position_ids=micro_batch[\"position_ids\"],\n            max_seq_len=micro_batch[\"max_seq_len\"],\n            attn_bias=BlockDiagonalCausalMask.from_seqlens(micro_batch[\"seq_lens\"]),\n        )\n        seq_info = to_device(seq_info, \"cuda:0\")\n\n        if is_first:\n            # First micro-batch: run only forward_part1\n            with torch.no_grad(), torch.cuda.amp.autocast():\n                prev_hidden_states = model.forward_part1(input_ids, seq_info, inv_freq0)\n            is_first = False\n\n            # Move intermediate results to GPU 1 for next step\n            prev_seq_info, prev_hidden_states = to_device(\n                [seq_info, prev_hidden_states], \"cuda:1\"\n            )\n            continue\n\n        # Pipelined inference:\n        # - Run part2 for previous micro-batch on GPU 1\n        # - Run part1 for current micro-batch on GPU 0\n        with torch.no_grad(), torch.cuda.amp.autocast():\n            logits = model.forward_part2(prev_hidden_states, prev_seq_info, inv_freq1)\n            hidden_states = model.forward_part1(input_ids, seq_info, inv_freq0)\n\n            # Prepare next micro-batch state\n            prev_seq_info, prev_hidden_states = to_device(\n                [seq_info, hidden_states], \"cuda:1\"\n            )\n            outs.append(logits.cpu())  # Save logits to CPU memory\n\n# --- Process final micro-batch (no part1 needed) ---\nwith torch.no_grad(), torch.cuda.amp.autocast():\n    logits = model.forward_part2(prev_hidden_states, prev_seq_info, inv_freq1)\n    outs.append(logits.cpu())\n\n# --- Evaluate and Save ---\npred = torch.cat(outs, dim=0)         # Concatenate logits\nprob = pred.softmax(-1)               # Convert logits to probabilities\nprint(dataset.evaluate(prob.numpy())) # Evaluate predictions\n\nnp.save('prob_m3.npy', prob)          # Save prediction scores\nprint(\"prob_m3 saved\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T05:09:45.802172Z","iopub.execute_input":"2025-11-06T05:09:45.802478Z","iopub.status.idle":"2025-11-06T05:09:45.811355Z","shell.execute_reply.started":"2025-11-06T05:09:45.802452Z","shell.execute_reply":"2025-11-06T05:09:45.810307Z"}},"outputs":[{"name":"stdout","text":"Overwriting llama.py\n","output_type":"stream"}],"execution_count":97},{"cell_type":"code","source":"!python llama.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T05:09:45.812579Z","iopub.execute_input":"2025-11-06T05:09:45.812842Z","iopub.status.idle":"2025-11-06T05:14:08.448316Z","shell.execute_reply.started":"2025-11-06T05:09:45.812821Z","shell.execute_reply":"2025-11-06T05:14:08.446997Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nLoading checkpoint shards: 100%|██████████████████| 4/4 [04:07<00:00, 61.85s/it]\n  0%|                                                     | 0/1 [00:00<?, ?it/s]2025-11-06 05:14:02.093794: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2025-11-06 05:14:02.093877: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-11-06 05:14:02.096098: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n100%|█████████████████████████████████████████████| 1/1 [00:03<00:00,  4.00s/it]\n{'log_loss': 0.7582519183970864}\nprob_m3 saved\n","output_type":"stream"}],"execution_count":98},{"cell_type":"code","source":"import numpy as np\n\nprob = np.load('prob_m3.npy')\n\nprint(prob[:5])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T05:14:08.450063Z","iopub.execute_input":"2025-11-06T05:14:08.450393Z","iopub.status.idle":"2025-11-06T05:14:08.458084Z","shell.execute_reply.started":"2025-11-06T05:14:08.450365Z","shell.execute_reply":"2025-11-06T05:14:08.456879Z"}},"outputs":[{"name":"stdout","text":"[[0.98613375 0.0021362  0.01173002]\n [0.13743691 0.5760938  0.28646934]\n [0.7586595  0.09096359 0.1503769 ]]\n","output_type":"stream"}],"execution_count":99},{"cell_type":"markdown","source":"# Inference: Faiss","metadata":{}},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom transformers import AutoModel\nfrom transformers import AutoTokenizer\nfrom transformers import AutoTokenizer\nfrom sklearn.preprocessing import MinMaxScaler\nimport faiss\n\nmodel_load_path = '/kaggle/input/some-pack/sentence-transformer-model' \nsentence_model = SentenceTransformer(model_load_path)\n\ntest_data = pd.read_csv('/kaggle/input/llm-classification-finetuning/test.csv')\n\nclass CustomDebertaModel(nn.Module):\n    def __init__(self, model_name, num_labels, feature_dim=2, dropout_rate=0.1):\n        super(CustomDebertaModel, self).__init__()\n        \n        # Initialize DeBERTa model\n        self.base_model = AutoModel.from_pretrained(model_name)\n        \n        # Feature tower for similarity features (a small MLP)\n        self.feature_fc = nn.Sequential(\n            nn.Linear(feature_dim, 128),                 # Map input similarity features to 128-dimensional space\n            nn.ReLU(),\n            nn.Dropout(p=dropout_rate),\n            nn.Linear(128, self.base_model.config.hidden_size),  # Project to same size as text embeddings\n            nn.ReLU()\n        )\n        \n        # Attention mechanism to allow interaction between text and similarity embeddings\n        self.attention = nn.MultiheadAttention(\n            embed_dim=self.base_model.config.hidden_size,\n            num_heads=4,  # Number of attention heads\n            batch_first=True  # Enable batch-first input format\n        )\n        \n        # Dropout layer for regularization\n        self.dropout = nn.Dropout(p=dropout_rate)\n\n        # Final classifier layer (MLP for classification)\n        self.classifier = nn.Sequential(\n            nn.Linear(self.base_model.config.hidden_size * 2, self.base_model.config.hidden_size),  # Combine text + attention features\n            nn.ReLU(),\n            nn.Dropout(p=dropout_rate),\n            nn.Linear(self.base_model.config.hidden_size, num_labels)\n        )\n\n    def forward(self, input_ids, attention_mask, similarity_features, labels=None):\n        # Text tower: extract [CLS] token embedding from DeBERTa\n        base_outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n        text_embeddings = base_outputs.last_hidden_state[:, 0, :]  # [batch_size, hidden_size]\n\n        # Feature tower: process similarity features through MLP\n        similarity_embeds = self.feature_fc(similarity_features)  # [batch_size, hidden_size]\n\n        # Cross-modal interaction using attention mechanism\n        query = text_embeddings.unsqueeze(1)       # Shape: [batch_size, 1, hidden_size]\n        key_value = similarity_embeds.unsqueeze(1) # Shape: [batch_size, 1, hidden_size]\n        attention_output, _ = self.attention(query, key_value, key_value)  # [batch_size, 1, hidden_size]\n\n        # Concatenate text and attended similarity features\n        combined_features = torch.cat([text_embeddings, attention_output.squeeze(1)], dim=1)\n\n        # Apply dropout and classification head\n        logits = self.classifier(self.dropout(combined_features))\n\n        # Output dictionary with logits\n        outputs = {\"logits\": logits}\n        \n        # If labels are provided (e.g., during training), compute the cross-entropy loss\n        if labels is not None:\n            loss_fn = nn.CrossEntropyLoss()\n            outputs[\"loss\"] = loss_fn(logits, labels)\n\n        return outputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T05:14:08.459941Z","iopub.execute_input":"2025-11-06T05:14:08.460336Z","iopub.status.idle":"2025-11-06T05:14:09.747676Z","shell.execute_reply.started":"2025-11-06T05:14:08.460301Z","shell.execute_reply":"2025-11-06T05:14:09.746754Z"}},"outputs":[],"execution_count":100},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nimport faiss\n\ntest_data = pd.read_csv('/kaggle/input/llm-classification-finetuning/test.csv')\n\n# Compute semantic similarity scores using FAISS\ndef compute_semantic_features_with_faiss(df):\n    # Extract prompts and responses as lists\n    prompts = df['prompt'].tolist()\n    responses_a = df['response_a'].tolist()\n    responses_b = df['response_b'].tolist()\n\n    # Generate sentence embeddings and normalize them (unit vectors)\n    prompt_embeddings = np.array(sentence_model.encode(prompts))\n    prompt_embeddings = prompt_embeddings / np.linalg.norm(prompt_embeddings, axis=1, keepdims=True)\n\n    response_a_embeddings = np.array(sentence_model.encode(responses_a))\n    response_a_embeddings = response_a_embeddings / np.linalg.norm(response_a_embeddings, axis=1, keepdims=True)\n\n    response_b_embeddings = np.array(sentence_model.encode(responses_b))\n    response_b_embeddings = response_b_embeddings / np.linalg.norm(response_b_embeddings, axis=1, keepdims=True)\n\n    # Determine the embedding dimension\n    dim = prompt_embeddings.shape[1]\n    \n    # Create a FAISS index using inner product (cosine similarity since vectors are normalized)\n    index_flat = faiss.IndexFlatIP(dim)\n\n    # Compute similarity between response A and prompt\n    index_flat.add(prompt_embeddings)  # Add prompt embeddings to the FAISS index\n    similarity_a = index_flat.search(response_a_embeddings, k=1)[0].squeeze()  # Get top-1 similarity score\n\n    # Reset and compute similarity for response B\n    index_flat.reset()\n    index_flat.add(prompt_embeddings)\n    similarity_b = index_flat.search(response_b_embeddings, k=1)[0].squeeze()\n\n    # Store similarity scores in the DataFrame\n    df['similarity_a'] = similarity_a\n    df['similarity_b'] = similarity_b\n\n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T05:14:09.748951Z","iopub.execute_input":"2025-11-06T05:14:09.749639Z","iopub.status.idle":"2025-11-06T05:14:09.758527Z","shell.execute_reply.started":"2025-11-06T05:14:09.749597Z","shell.execute_reply":"2025-11-06T05:14:09.757772Z"}},"outputs":[],"execution_count":101},{"cell_type":"code","source":"test_data = compute_semantic_features_with_faiss(test_data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T05:14:09.759429Z","iopub.execute_input":"2025-11-06T05:14:09.759719Z","iopub.status.idle":"2025-11-06T05:14:09.853400Z","shell.execute_reply.started":"2025-11-06T05:14:09.759689Z","shell.execute_reply":"2025-11-06T05:14:09.852633Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e7628a8faac44fdaeb689503ff12d6b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9fa5078ecec41bcae0d3dd518c595d7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc0296111da5431ba0380693d263ef52"}},"metadata":{}}],"execution_count":102},{"cell_type":"code","source":"# Load the trained custom PyTorch model\nmodel = torch.load(\"/kaggle/input/akemiiiiii/custom_model_dir/custom_model_complete.pth\")\n\n# Set device to GPU if available, otherwise fallback to CPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\nmodel.eval()  # Important: set to evaluation mode\n\nprint(\"Custom model loaded successfully!\")\n\n# Load the tokenizer used during training\ntokenizer_path = \"/kaggle/input/akemiiiiii/custom_model_dir\"\ntokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n\n# Function to preprocess a single test sample\ndef preprocess_test_data(row):\n    # Construct the input string in the same format used for training\n    input_text = f\"Prompt: {row['prompt']} Response A: {row['response_a']} Response B: {row['response_b']}\"\n\n    # Tokenize the input using the same tokenizer settings as training\n    tokenized_inputs = tokenizer(\n        input_text,\n        truncation=True,\n        padding=\"max_length\",\n        max_length=512,\n        return_tensors=\"pt\"  # Return PyTorch tensors\n    )\n\n    # Add similarity scores as additional features\n    # They should be shaped as [1, feature_dim] to batch correctly\n    similarity = torch.tensor([[row[\"similarity_a\"], row[\"similarity_b\"]]], dtype=torch.float32)\n\n    tokenized_inputs[\"similarity_features\"] = similarity\n\n    return tokenized_inputs\n\n# Apply the preprocessing function to every row of the test dataset\nprocessed_test_data = [preprocess_test_data(row) for _, row in test_data.iterrows()]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T05:14:09.855453Z","iopub.execute_input":"2025-11-06T05:14:09.855696Z","iopub.status.idle":"2025-11-06T05:14:15.698935Z","shell.execute_reply.started":"2025-11-06T05:14:09.855675Z","shell.execute_reply":"2025-11-06T05:14:15.698161Z"}},"outputs":[{"name":"stdout","text":"Custom model loaded successfully!\n","output_type":"stream"}],"execution_count":103},{"cell_type":"code","source":"class TestDataset(torch.utils.data.Dataset):\n    def __init__(self, data):\n        self.data = data\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        return self.data[idx]\n\ntest_dataset = TestDataset(processed_test_data)\n\ndef collate_fn_test(batch):\n    # Concatenate input_ids and attention_mask along batch dimension\n    input_ids = torch.cat([item[\"input_ids\"] for item in batch], dim=0)\n    attention_mask = torch.cat([item[\"attention_mask\"] for item in batch], dim=0)\n    \n    # Stack similarity features (already shape [1, 2])\n    similarity_features = torch.cat([item[\"similarity_features\"] for item in batch], dim=0)  # shape [B, 2]\n\n    return {\n        \"input_ids\": input_ids,\n        \"attention_mask\": attention_mask,\n        \"similarity_features\": similarity_features,\n    }\n\n# DataLoader\ntest_dataloader = DataLoader(test_dataset, batch_size=8, collate_fn=collate_fn_test, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T05:14:15.700342Z","iopub.execute_input":"2025-11-06T05:14:15.700595Z","iopub.status.idle":"2025-11-06T05:14:15.708126Z","shell.execute_reply.started":"2025-11-06T05:14:15.700574Z","shell.execute_reply":"2025-11-06T05:14:15.707128Z"}},"outputs":[],"execution_count":104},{"cell_type":"code","source":"# Set the model to evaluation mode (important to deactivate dropout, etc.)\nmodel.eval()\n\n# List to store prediction results\npredictions = []\n\n# Disable gradient calculation (saves memory and speeds up inference)\nwith torch.no_grad():\n    for batch in test_dataloader:\n        # Move batch data to the correct device (GPU or CPU)\n        input_ids = batch[\"input_ids\"].to(device)\n        attention_mask = batch[\"attention_mask\"].to(device)\n        similarity_features = batch[\"similarity_features\"].to(device)\n\n        # Run forward pass (model inference)\n        outputs = model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            similarity_features=similarity_features\n        )\n\n        # Extract raw logits from output\n        logits = outputs[\"logits\"]\n\n        # Convert logits to probabilities using softmax\n        probs = torch.nn.functional.softmax(logits, dim=-1)\n\n        # Move to CPU and convert to NumPy for later use\n        predictions.append(probs.cpu().numpy())\n\n# Concatenate all batch results into a single NumPy array\npredictions = np.concatenate(predictions, axis=0)\n\n# Print the final predictions\nprint(predictions)\n\n# Save predictions to a .npy file for downstream analysis or evaluation\nnp.save('prob_faiss.npy', predictions)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T05:14:15.709501Z","iopub.execute_input":"2025-11-06T05:14:15.709859Z","iopub.status.idle":"2025-11-06T05:14:15.835862Z","shell.execute_reply.started":"2025-11-06T05:14:15.709827Z","shell.execute_reply":"2025-11-06T05:14:15.834859Z"}},"outputs":[{"name":"stdout","text":"[[0.18443021 0.22523855 0.59033126]\n [0.41683194 0.3362972  0.24687083]\n [0.36593515 0.37464315 0.25942168]]\n","output_type":"stream"}],"execution_count":105},{"cell_type":"markdown","source":"# Create submission file","metadata":{}},{"cell_type":"code","source":"df = pd.read_parquet(\"test.parquet\")\n\n\nprob_m0 = np.load(\"prob_m0.npy\")  # Gemma2\nprob_m3 = np.load(\"prob_m3.npy\")[:, [1, 0, 2]]  # Llama3 (swap response_a and response_b)\nprob_faiss = np.load(\"prob_faiss.npy\")  # faiss\n\n# Combine predictions with weights\n# Adjust weights as needed for optimal performance\npreds = np.average(\n    [\n        prob_m0,       # Gemma2 results\n        prob_m3,       # Llama3 results\n        prob_faiss     # faiss results\n    ],\n    axis=0,\n    weights=[0.5, 0.5, 0.0]  # Weights for each model\n)\n\n# Create submission DataFrame\nsub = pd.DataFrame({\n    \"id\": df[\"id\"],\n    \"winner_model_a\": preds[:, 0],\n    \"winner_model_b\": preds[:, 1],\n    \"winner_tie\": preds[:, 2],\n})\n\n# Save to CSV\nsub.to_csv(\"submission.csv\", index=False)\nprint(sub.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T05:14:15.837187Z","iopub.execute_input":"2025-11-06T05:14:15.837596Z","iopub.status.idle":"2025-11-06T05:14:15.860391Z","shell.execute_reply.started":"2025-11-06T05:14:15.837560Z","shell.execute_reply":"2025-11-06T05:14:15.859334Z"}},"outputs":[{"name":"stdout","text":"        id  winner_model_a  winner_model_b  winner_tie\n0   136060        0.002239        0.982338    0.015423\n1   211333        0.533100        0.129717    0.337183\n2  1233961        0.085959        0.764494    0.149547\n","output_type":"stream"}],"execution_count":106},{"cell_type":"markdown","source":"# Reference\n- LMSYS - Chatbot Arena Human Preference Predictions 2nd place solution - https://www.kaggle.com/competitions/lmsys-chatbot-arena/discussion/527685\n- Blue - https://www.kaggle.com/code/blue0924/finetuning-test2","metadata":{}}]}