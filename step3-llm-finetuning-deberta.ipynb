{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":86518,"databundleVersionId":9809560,"sourceType":"competition"},{"sourceId":13558864,"sourceType":"datasetVersion","datasetId":8612406},{"sourceId":13586220,"sourceType":"datasetVersion","datasetId":8631782}],"dockerImageVersionId":31153,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n!pip install peft transformers -q\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-02T12:41:51.327768Z","iopub.execute_input":"2025-11-02T12:41:51.328187Z","iopub.status.idle":"2025-11-02T12:41:56.524593Z","shell.execute_reply.started":"2025-11-02T12:41:51.328152Z","shell.execute_reply":"2025-11-02T12:41:56.523050Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import the data\ntraining = pd.read_csv('/kaggle/input/llm-classification-finetuning/train.csv')\ntest = pd.read_csv('/kaggle/input/llm-classification-finetuning/test.csv')\n\ntraining['train_test'] = 1\ntest['train_test'] = 0\nall_data = pd.concat([training,test])\n\nprint(\"Import Data Complete\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T12:41:56.527499Z","iopub.execute_input":"2025-11-02T12:41:56.527944Z","iopub.status.idle":"2025-11-02T12:41:59.758350Z","shell.execute_reply.started":"2025-11-02T12:41:56.527909Z","shell.execute_reply":"2025-11-02T12:41:59.756872Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"training.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T12:41:59.759453Z","iopub.execute_input":"2025-11-02T12:41:59.759794Z","iopub.status.idle":"2025-11-02T12:41:59.824799Z","shell.execute_reply.started":"2025-11-02T12:41:59.759765Z","shell.execute_reply":"2025-11-02T12:41:59.823501Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"training.head(10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T12:41:59.829622Z","iopub.execute_input":"2025-11-02T12:41:59.830048Z","iopub.status.idle":"2025-11-02T12:41:59.877659Z","shell.execute_reply.started":"2025-11-02T12:41:59.829990Z","shell.execute_reply":"2025-11-02T12:41:59.876235Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"training.tail(10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T12:41:59.879222Z","iopub.execute_input":"2025-11-02T12:41:59.879602Z","iopub.status.idle":"2025-11-02T12:41:59.910043Z","shell.execute_reply.started":"2025-11-02T12:41:59.879559Z","shell.execute_reply":"2025-11-02T12:41:59.908750Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#check for duplicate id's\ntotal_id = len(training[\"id\"])\ntotal_unique_id = len(training[\"id\"].unique())\n\nprint(\"Total number of 'id' duplicates:\")\nprint(total_id - total_unique_id)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T12:41:59.911578Z","iopub.execute_input":"2025-11-02T12:41:59.911931Z","iopub.status.idle":"2025-11-02T12:41:59.937363Z","shell.execute_reply.started":"2025-11-02T12:41:59.911899Z","shell.execute_reply":"2025-11-02T12:41:59.935924Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#check for null or empty cells\nnan_count = training.isna().sum().sum()\nnull_count = training.isnull().sum().sum()\n\nprint('Number of NaN values:', nan_count)\nprint('Number of null values:', null_count)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T12:41:59.938939Z","iopub.execute_input":"2025-11-02T12:41:59.939424Z","iopub.status.idle":"2025-11-02T12:42:00.028388Z","shell.execute_reply.started":"2025-11-02T12:41:59.939393Z","shell.execute_reply":"2025-11-02T12:42:00.027272Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#check for consistency of model_a, model_b and LLMs identification\ntotal_unique_model_a = len(training[\"model_a\"].unique())\ntotal_unique_model_b = len(training[\"model_b\"].unique())\n\nprint(\"Total number of 'model_a' and 'model_b' unique values:\")\nprint('model_a =', total_unique_model_a)\nprint('model_b =', total_unique_model_b)\n\nLLM_a = training[\"model_a\"].unique()\nLLM_b = training[\"model_b\"].unique()\nLLM = list(set(LLM_a).intersection(set(LLM_b)))\n\nprint(\"total number of LLMs =\", len(LLM))\nprint('LLMs utilized:', LLM)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T12:42:00.029667Z","iopub.execute_input":"2025-11-02T12:42:00.030190Z","iopub.status.idle":"2025-11-02T12:42:00.057707Z","shell.execute_reply.started":"2025-11-02T12:42:00.030153Z","shell.execute_reply":"2025-11-02T12:42:00.056082Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#check for duplicate prompts -  the same prompt could have been given to more than two different LLMs\ntotal_prompt = len(training[\"prompt\"])\ntotal_unique_prompt = len(training[\"prompt\"].unique())\n\nprint(\"Total number of 'prompt' duplicates:\")\nprint(total_prompt - total_unique_prompt)\n\n#There are 57477 observations and 5743 prompt duplicates without id duplicates -> having prompt duplicates are ok to have \n#and no further data cleaning is needed to deal with prompt duplicates","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T12:42:00.058816Z","iopub.execute_input":"2025-11-02T12:42:00.059184Z","iopub.status.idle":"2025-11-02T12:42:00.147661Z","shell.execute_reply.started":"2025-11-02T12:42:00.059152Z","shell.execute_reply":"2025-11-02T12:42:00.145926Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import re\nimport nltk\nfrom nltk.corpus import stopwords\n#clean cells with text data\ndef preprocess_text(text):\n    #convert text to lower case\n    text = text.lower()\n    #remove digits and special characters using regular expressions\n    text = re.sub(r'\\d+', '', text)\n    text = re.sub(r'[^\\w\\s]', '', text)\n    #tokenize the text\n    text = nltk.word_tokenize(text)\n    \n    return text\n\ndef remove_stopwords(text):\n    #remove stopwords\n    stop_words = set(stopwords.words('english'))\n    text_no_stopwords = [word for word in text if word not in stop_words]\n\n    return text_no_stopwords\n\n\ndef lemmatization(text):\n    lemmatizer = nltk.WordNetLemmatizer()\n    lemmatizer_text = [lemmatizer.lemmatize(text) for text in text]\n\n    return lemmatizer_text\n\n\ndef clean_text(text):\n    #convert text to lower case, remove digits and special characters using regular expressions and remove stopwords combined together\n    text = preprocess_text(text)\n    filtered_text = remove_stopwords(text)\n    lemmatizer_text = lemmatization(filtered_text)\n    clean_text = ' '.join(lemmatizer_text)\n\n    return clean_text\n\ntraining[\"prompt\"] = training[\"prompt\"].apply(clean_text)\ntraining[\"response_a\"] = training[\"response_a\"].apply(clean_text)\ntraining[\"response_b\"] = training[\"response_b\"].apply(clean_text)\n\n\nprint(\"Clean Cells with Text Data Complete\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T12:42:00.151681Z","iopub.execute_input":"2025-11-02T12:42:00.152048Z","iopub.status.idle":"2025-11-02T12:45:41.840271Z","shell.execute_reply.started":"2025-11-02T12:42:00.151992Z","shell.execute_reply":"2025-11-02T12:45:41.838564Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n#barchart - model a\nresult_model_a = training[\"model_a\"].value_counts()\nprint(\"model a:\", result_model_a)\n\n## Matplotlib barchart:\nprint(\"-----\")\nprint(\"Matplotlib barchart, model a:\")\n  \nbarWidth = 0.45\nplt.figure(figsize=(15, 7))\n\nplt.bar(result_model_a.index, result_model_a.values, barWidth, color='r')\n\nplt.ylabel('Counts', fontweight ='bold', fontsize = 15)\nplt.xlabel('LLMs', fontweight ='bold', fontsize = 15)\n\nplt.xticks(rotation=90)\nplt.title('LLMs Value Counts - model a', fontweight ='bold', fontsize = 15)\n\n\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T12:45:41.841659Z","iopub.execute_input":"2025-11-02T12:45:41.842045Z","iopub.status.idle":"2025-11-02T12:45:42.554174Z","shell.execute_reply.started":"2025-11-02T12:45:41.841973Z","shell.execute_reply":"2025-11-02T12:45:42.553116Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#barchart - model b\nresult_model_b = training[\"model_b\"].value_counts()\nprint(\"model b:\", result_model_b)\n\n## Matplotlib barchart:\nprint(\"-----\")\nprint(\"Matplotlib barchart, model b:\")\n  \nbarWidth = 0.45\nplt.figure(figsize=(15, 7))\n\nplt.bar(result_model_b.index, result_model_b.values, barWidth, color='g')\n\nplt.ylabel('Counts', fontweight ='bold', fontsize = 15)\nplt.xlabel('LLMs', fontweight ='bold', fontsize = 15)\n\nplt.xticks(rotation=90)\nplt.title('LLMs Value Counts - model b', fontweight ='bold', fontsize = 15)\n\n\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T12:45:42.555314Z","iopub.execute_input":"2025-11-02T12:45:42.555599Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#barchart - model winner: winner model a, winner model b or winner tie\ndef which_winner(value):\n    if  value[\"winner_model_a\"] == 1:\n         #winner model a\n         value[\"winner_model_b\"] = 0\n         value[\"winner_tie\"] = 0\n         return 0\n    elif value[\"winner_model_b\"] == 1:\n         #winner model b\n         return 1\n    elif value[\"winner_tie\"] == 1:\n         #winner tie\n         return 2\n    return None\n\ntraining[\"winner\"] = training.apply(which_winner, axis=1)\n\ntraining[\"winner_model\"] = training[\"winner\"].astype(str)\ntraining.loc[training[\"winner_model\"] == \"0\", \"winner_model\"] = \"model a\"\ntraining.loc[training[\"winner_model\"] == \"1\", \"winner_model\"] = \"model b\"\ntraining.loc[training[\"winner_model\"] == \"2\", \"winner_model\"] = \"winner tie\"\n\nresult_model_winner = training[\"winner_model\"].value_counts()\nprint(\"model winner:\", result_model_winner)\n\nprint(\"-----\")\nprint(\"Matplotlib barchart, model winner:\")\n\nbarWidth = 0.75\nplt.figure(figsize=(8, 7))\n\nplt.bar(result_model_winner.index, result_model_winner.values, barWidth, color='b')\n\nplt.ylabel('Counts', fontweight ='bold', fontsize = 15)\nplt.xlabel('Model winner', fontweight ='bold', fontsize = 15)\n\nplt.title('LLMs Value Counts - model winner', fontweight ='bold', fontsize = 15)\n\n\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.idle":"2025-11-02T12:45:44.480185Z","shell.execute_reply.started":"2025-11-02T12:45:43.288235Z","shell.execute_reply":"2025-11-02T12:45:44.479103Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom peft import PeftModel, LoraConfig, TaskType\nfrom datasets import Dataset\n\n# NUM_LABELS는 승자 클래스 개수: (0: model a, 1: model b, 2: winner tie)\nNUM_LABELS = 3 \n# DeBERTa 기본 모델 경로 (사전 학습 가중치)\nBASE_MODEL_PATH = \"/kaggle/input/deberta-v3-base-for-llm-comp/deberta-v3-base-local\" \n# LoRA 어댑터 가중치 경로 (파인튜닝된 가벼운 가중치)\nLORA_WEIGHTS_PATH = \"/kaggle/input/deberta-lora-weight-2/deberta_lora_weights\"\n\ntrain_df = training.copy()\ntrain_df[\"text\"] = train_df[\"prompt\"] + \" [SEP] \" + train_df[\"response_a\"] + \" [SEP] \" + train_df[\"response_b\"]\ntrain_df[\"labels\"] = train_df[\"winner\"].astype(int)\n\nhf_train_dataset = Dataset.from_pandas(train_df[['text', 'labels', 'id']])\n\ntokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH)\nMAX_LENGTH = 512\n\ndef tokenize_function(examples):\n    return tokenizer(examples[\"text\"], truncation=True, max_length=MAX_LENGTH)\n\ntokenized_train_dataset = hf_train_dataset.map(tokenize_function, batched=True, remove_columns=['text'])\ntokenized_train_dataset.set_format(\"torch\")\n\nprint(\"DeBERTa 데이터 토큰화 완료\")\n\nbase_model = AutoModelForSequenceClassification.from_pretrained(\n    BASE_MODEL_PATH,\n    num_labels=NUM_LABELS,\n    ignore_mismatched_sizes=True \n)\n\nmodel = PeftModel.from_pretrained(base_model, LORA_WEIGHTS_PATH)\nmodel.eval() \n\nprint(\"LoRA DeBERTa 모델 로드 및 추론 모드 설정 완료\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T12:45:44.481208Z","iopub.execute_input":"2025-11-02T12:45:44.481490Z","iopub.status.idle":"2025-11-02T12:47:55.775500Z","shell.execute_reply.started":"2025-11-02T12:45:44.481469Z","shell.execute_reply":"2025-11-02T12:47:55.774191Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import Trainer, TrainingArguments\nimport numpy as np\nimport os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nos.environ[\"OMP_NUM_THREADS\"] = \"1\"\nos.environ[\"WANDB_DISABLED\"] = \"true\"\nDATALOADER_NUM_WORKERS = 0\n\ntest_df = test.copy() \ntest_df[\"text\"] = test_df[\"prompt\"] + \" [SEP] \" + test_df[\"response_a\"] + \" [SEP] \" + test_df[\"response_b\"]\n\nhf_test_dataset = Dataset.from_pandas(test_df[['text', 'id']])\ntokenized_test_dataset = hf_test_dataset.map(tokenize_function, batched=True, remove_columns=['text'])\ntokenized_test_dataset.set_format(\"torch\")\n\n#Trainer를 사용한 추론 (Model Prediction)\nINFERENCE_BATCH_SIZE = 128\ninference_args = TrainingArguments(\n    output_dir=\"./deberta_inference_results\",\n    per_device_eval_batch_size=INFERENCE_BATCH_SIZE, \n    dataloader_num_workers=DATALOADER_NUM_WORKERS,    \n    fp16=True, \n    # report_to=\"none\"\n)\nmodel = model.to('cpu').eval()\n\ntrainer = Trainer(\n    model=model,\n    args=inference_args,\n    tokenizer=tokenizer,\n)\n\n# 추론 실행 (predict_proba와 동일)\nraw_predictions = trainer.predict(tokenized_test_dataset)\n\n# 확률값 추출 (로짓(logits)을 softmax를 통해 확률로 변환)\nlogits = raw_predictions.predictions\nprobabilities = torch.nn.functional.softmax(torch.tensor(logits), dim=-1).numpy()\n\nvalue_test_y_probabilities = probabilities \n\nprint('Model winner prediction, probability', value_test_y_probabilities)\n\noutput = pd.DataFrame({\n    'id': test_df.id.values,\n    'winner_model_a': value_test_y_probabilities[:, 0],\n    'winner_model_b': value_test_y_probabilities[:, 1],\n    'winner_tie': value_test_y_probabilities[:, 2]\n})\n\noutput.to_csv('submission.csv', index=False)\n\nprint(\"DeBERTa LoRA 추론 및 Submission 파일 생성 완료\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T12:47:55.777055Z","iopub.execute_input":"2025-11-02T12:47:55.777415Z","iopub.status.idle":"2025-11-02T12:48:00.729643Z","shell.execute_reply.started":"2025-11-02T12:47:55.777389Z","shell.execute_reply":"2025-11-02T12:48:00.728191Z"}},"outputs":[],"execution_count":null}]}