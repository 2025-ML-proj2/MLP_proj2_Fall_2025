{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":86518,"databundleVersionId":9809560,"sourceType":"competition"},{"sourceId":8897601,"sourceType":"datasetVersion","datasetId":5297895},{"sourceId":9102725,"sourceType":"datasetVersion","datasetId":5493674},{"sourceId":9107824,"sourceType":"datasetVersion","datasetId":5496762},{"sourceId":9107963,"sourceType":"datasetVersion","datasetId":5496847},{"sourceId":9108069,"sourceType":"datasetVersion","datasetId":5496920},{"sourceId":10214229,"sourceType":"datasetVersion","datasetId":6267026},{"sourceId":10302322,"sourceType":"datasetVersion","datasetId":6265823},{"sourceId":75103,"sourceType":"modelInstanceVersion","modelInstanceId":63082,"modelId":86587}],"dockerImageVersionId":30746,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Installing Packages","metadata":{}},{"cell_type":"code","source":"# 주의사항: 항상 GPU T4 x2를 사용해야 함\nimport torch\nprint(torch.cuda.is_available())\nprint(torch.cuda.device_count())\nprint(torch.cuda.get_device_name(0))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T12:34:33.405654Z","iopub.execute_input":"2025-11-01T12:34:33.405901Z","iopub.status.idle":"2025-11-01T12:34:38.842157Z","shell.execute_reply.started":"2025-11-01T12:34:33.405877Z","shell.execute_reply":"2025-11-01T12:34:38.841204Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 외부 패키지 import\n%pip install /kaggle/input/lmsys-packages/triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n%pip install /kaggle/input/lmsys-packages/xformers-0.0.24042abc8.d20240802-cp310-cp310-linux_x86_64.whl\n%pip install /kaggle/input/some-pack/faiss_cpu_downloads/faiss_cpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n%pip install --no-index --find-links=/kaggle/input/some-pack/sentence_transformers_packages sentence-transformers","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!cp -r /kaggle/input/lmsys-modules-0805 human_pref","metadata":{"execution":{"iopub.status.busy":"2025-11-01T12:37:04.603590Z","iopub.execute_input":"2025-11-01T12:37:04.604461Z","iopub.status.idle":"2025-11-01T12:37:05.769547Z","shell.execute_reply.started":"2025-11-01T12:37:04.604406Z","shell.execute_reply":"2025-11-01T12:37:05.768578Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Check input files","metadata":{}},{"cell_type":"code","source":"import os\n\n# /kaggle/input 폴더 아래 모든 디렉터리/파일 순회\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    # 현재 디렉토리의 각 파일 순회\n    for filename in filenames:\n        # 디렉터리와 파일명 연결해 파일의 전체 경로 출력\n        print(os.path.join(dirname, filename))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T12:37:05.770916Z","iopub.execute_input":"2025-11-01T12:37:05.771212Z","iopub.status.idle":"2025-11-01T12:37:05.910717Z","shell.execute_reply.started":"2025-11-01T12:37:05.771188Z","shell.execute_reply":"2025-11-01T12:37:05.909948Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Prepare test file","metadata":{}},{"cell_type":"code","source":"%%writefile test.py\nimport pandas as pd\n\n# Kaggle 입력 디렉터리에서 원본 테스트 CSV 파일 로드\ndf = pd.read_csv(\"/kaggle/input/llm-classification-finetuning/test.csv\")\n\n# 모델 A가 항상 승자임을 나타내는 더미 레이블 열 추가\ndf[\"winner_model_a\"] = 1\ndf[\"winner_model_b\"] = 0\ndf[\"winner_tie\"] = 0\n\n## TODO: 어째서 이렇게 하는가?\n# 원본 테스트 데이터프레임(가짜 레이블 포함)을 parquet 파일로 저장\ndf.to_parquet(\"test.parquet\", index=False)\n\n# 응답 A와 B 교환(역방향 비교 시뮬레이션)\ndf[\"response_a\"], df[\"response_b\"] = df[\"response_b\"], df[\"response_a\"]\n\n# 스왑된 버전을 별도의 parquet 파일로 저장\ndf.to_parquet(\"test_swap.parquet\", index=False)","metadata":{"execution":{"iopub.status.busy":"2025-11-01T12:37:05.911710Z","iopub.execute_input":"2025-11-01T12:37:05.911958Z","iopub.status.idle":"2025-11-01T12:37:05.917915Z","shell.execute_reply.started":"2025-11-01T12:37:05.911938Z","shell.execute_reply":"2025-11-01T12:37:05.916991Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python test.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T12:37:05.920311Z","iopub.execute_input":"2025-11-01T12:37:05.920554Z","iopub.status.idle":"2025-11-01T12:37:07.848874Z","shell.execute_reply.started":"2025-11-01T12:37:05.920535Z","shell.execute_reply":"2025-11-01T12:37:07.847863Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Inference: gemma2-9b","metadata":{}},{"cell_type":"code","source":"%%writefile gemma.py\nimport torch\nimport numpy as np\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nfrom transformers import AutoTokenizer\n\nfrom xformers.ops.fmha.attn_bias import BlockDiagonalCausalMask\nfrom human_pref.models.modeling_gemma2 import Gemma2ForSequenceClassification\nfrom human_pref.data.processors import ProcessorPAB\nfrom human_pref.data.dataset import LMSYSDataset\nfrom human_pref.data.collators import VarlenCollator, ShardedMaxTokensCollator\nfrom human_pref.utils import to_device\n\n# 사전 훈련된 모델 위치+입력 데이터 경로\nmodel_name_or_path = \"/kaggle/input/lmsys-checkpoints-0-0805\"  \ncsv_path = \"test.parquet\"  \n\n# 토크나이저와 프로세서 로드\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n\n# 입력 샘플(프롬프트-응답 쌍) 토큰화하는 프로세서\nprocessor = ProcessorPAB(\n    tokenizer=tokenizer,\n    max_length=5120,\n    support_system_role=False,\n)\n\n# 데이터셋 로드하고 전처리 적용\ndataset = LMSYSDataset(\n    csv_file=csv_path,\n    query=None,\n    processor=processor,\n    include_swap=False,\n    is_parquet=True,\n)\n\n# 토큰 총 개수에 따라 예제를 배치 처리하는 사용자 정의 정렬기를 사용한 DataLoader\n# 각 배치는 마이크로 배치들의 목록\n# max_tokens: 배치당 최대 토큰 총량\ndataloader = DataLoader(\n    dataset,\n    batch_size=80,  \n    num_workers=4,\n    collate_fn=ShardedMaxTokensCollator(\n        max_tokens=8192,\n        base_collator=VarlenCollator()\n    ),\n)\n\n# GPU 2개 파이프라인 병렬 처리 정의\n# GPU 2개의 멀티프로세싱 성능을 극대화하기 위한 작업\n\n# 모델 내 트랜스포머 레이어의 수\nnum_hidden_layers = 42\n\n# 임베딩/최종 레이어/스코어 헤드를 GPU 0과 1에 할당\ndevice_map = {\n    \"model.embed_tokens\": \"cuda:0\",\n    \"model.norm\": \"cuda:1\",\n    \"score\": \"cuda:1\",\n}\n\n# 모델 레이어를 두 개의 GPU로 분할(첫 절반은 cuda:0, 두 번째 절반은 cuda:1)\nfor i in range(num_hidden_layers // 2):\n    device_map[f\"model.layers.{i}\"] = \"cuda:0\"\nfor i in range(num_hidden_layers // 2, num_hidden_layers):\n    device_map[f\"model.layers.{i}\"] = \"cuda:1\"\n\n# 가중치가 포함된 모델을 장치에 로드하고 float16을 사용\nmodel = Gemma2ForSequenceClassification.from_pretrained(\n    model_name_or_path,\n    torch_dtype=torch.float16,\n    device_map=device_map,\n)\n\n# 회전 임베딩 준비\n\nconfig = model.config\n# 어텐션 헤드의 차원\ndim = config.head_dim\n# RoPE(회전 위치 인코딩)에 대한 역주파수 계산\ninv_freq = 1.0 / (\n    config.rope_theta ** (torch.arange(0, dim, 2, dtype=torch.float32) / dim)\n)\n# 각각 모델 초반, 후반\ninv_freq0 = inv_freq.to(\"cuda:0\")\ninv_freq1 = inv_freq.to(\"cuda:1\")\n\n# 파이프라인 실행을 이용한 추론 루프\nis_first = True\nhidden_states = None\nouts = []\n\n# 전체 배치에 대해서\nfor batch in tqdm(dataloader):\n    # 각 배치는 마이크로배치의 리스트\n    for micro_batch in batch:\n        # 입력 토큰을 GPU 0으로 이동\n        input_ids = to_device(micro_batch[\"input_ids\"], \"cuda:0\")\n\n        # 시퀀스 관련 정보 준비\n        seq_info = dict(\n            cu_seqlens=micro_batch[\"cu_seqlens\"],\n            position_ids=micro_batch[\"position_ids\"],\n            max_seq_len=micro_batch[\"max_seq_len\"],\n            attn_bias=BlockDiagonalCausalMask.from_seqlens(micro_batch[\"seq_lens\"]),\n        )\n        seq_info = to_device(seq_info, \"cuda:0\")\n\n        # 첫 실행에서는 파트 1만 실행하고 상태 저장\n        if is_first:\n            with torch.no_grad(), torch.cuda.amp.autocast():\n                prev_hidden_states = model.forward_part1(input_ids, seq_info, inv_freq1)\n            is_first = False\n            # 다음 단계 위해 중간 출력을 GPU 1로 이동\n            prev_seq_info, prev_hidden_states = to_device(\n                [seq_info, prev_hidden_states], \"cuda:1\"\n            )\n            continue\n\n        # 이전 마이크로배치에 대해 2부 실행하고 현재에 대해 1부 실행\n        with torch.no_grad(), torch.cuda.amp.autocast():\n            # 이전 숨겨진 상태에 대한 최종 logit값 계산\n            # logit: 어떤 사건의 성공 확률을 나타내는 P(A)와 실패 확률을 나타내는 1-P(A)의 비율의 로그값\n            logits = model.forward_part2(prev_hidden_states, prev_seq_info, inv_freq1)\n\n            # 다음 마이크로배치에 대한 숨겨진 상태를 계산\n            hidden_states = model.forward_part1(input_ids, seq_info, inv_freq1)\n\n            # 새로운 숨겨진 상태와 seq_info를 GPU 1로 이동\n            prev_seq_info, prev_hidden_states = to_device(\n                [seq_info, hidden_states], \"cuda:1\"\n            )\n            # 예측 logit 값을 CPU에 저장\n            outs.append(logits.cpu())\n\n# 마지막 마이크로배치에 대한 최종 예측\nwith torch.no_grad(), torch.cuda.amp.autocast():\n    logits = model.forward_part2(prev_hidden_states, prev_seq_info, inv_freq1)\n    outs.append(logits.cpu())\n\n# 모든 logit을 연결하고 확률 계산\npred = torch.cat(outs, dim=0)\nprob = pred.softmax(-1)\n\n# 데이터셋의 내장 함수로 예측을 평가\nprint(dataset.evaluate(prob.numpy()))\n\n# 예측 확률을 파일에 저장\nnp.save('prob_m0.npy', prob)\n","metadata":{"execution":{"iopub.status.busy":"2025-11-01T12:37:07.850457Z","iopub.execute_input":"2025-11-01T12:37:07.850993Z","iopub.status.idle":"2025-11-01T12:37:07.859813Z","shell.execute_reply.started":"2025-11-01T12:37:07.850958Z","shell.execute_reply":"2025-11-01T12:37:07.858927Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python gemma.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T12:37:07.861333Z","iopub.execute_input":"2025-11-01T12:37:07.861729Z","iopub.status.idle":"2025-11-01T12:42:13.912795Z","shell.execute_reply.started":"2025-11-01T12:37:07.861696Z","shell.execute_reply":"2025-11-01T12:42:13.911713Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Inference: llama3-8b","metadata":{}},{"cell_type":"code","source":"%%writefile llama.py\nimport torch\nimport numpy as np\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nfrom transformers import AutoTokenizer\n\nfrom xformers.ops.fmha.attn_bias import BlockDiagonalCausalMask\nfrom human_pref.models.modeling_llama import LlamaForSequenceClassification\nfrom human_pref.data.processors import ProcessorPAB\nfrom human_pref.data.dataset import LMSYSDataset\nfrom human_pref.data.collators import VarlenCollator, ShardedMaxTokensCollator\nfrom human_pref.utils import to_device\n\n# 사전 훈련된 모델 위치+입력 데이터 경로\nmodel_name_or_path = \"/kaggle/input/lmsys-checkpoints-3-0805\"\ncsv_path = \"test_swap.parquet\"\n\n# 토크나이저와 프로세서 로드\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n\n# 토크나이저 경고 무시\ntokenizer.deprecation_warnings[\n    \"sequence-length-is-longer-than-the-specified-maximum\"\n] = True\n\n# 입력 샘플(프롬프트-응답 쌍) 토큰화하는 프로세서\nprocessor = ProcessorPAB(\n    tokenizer=tokenizer,\n    max_length=5120,\n    support_system_role=True,\n)\n\n# 데이터셋 로드하고 전처리 적용\ndataset = LMSYSDataset(\n    csv_file=csv_path,\n    query=None,\n    processor=processor,\n    include_swap=False,\n    is_parquet=True,\n)\n\n# 토큰 총 개수에 따라 예제를 배치 처리하는 사용자 정의 정렬기 사용한 DataLoader\ndataloader = DataLoader(\n    dataset,\n    batch_size=80,\n    num_workers=4,\n    collate_fn=ShardedMaxTokensCollator(\n        max_tokens=8192,\n        base_collator=VarlenCollator()\n    ),\n)\n\n# 파이프라인 병렬 처리를 위한 장치 매핑: LLaMA-3는 32개의 트랜스포머 레이어가 존재\nnum_hidden_layers = 32\n\ndevice_map = {\n    \"model.embed_tokens\": \"cuda:0\",\n    \"model.norm\": \"cuda:1\",\n    \"score\": \"cuda:1\",\n}\n\n# 모델 레이어를 두 개의 GPU로 분할(첫 절반은 cuda:0, 두 번째 절반은 cuda:1)\nfor i in range(num_hidden_layers // 2):\n    device_map[f\"model.layers.{i}\"] = \"cuda:0\"\nfor i in range(num_hidden_layers // 2, num_hidden_layers):\n    device_map[f\"model.layers.{i}\"] = \"cuda:1\"\n\n# 가중치가 포함된 모델을 장치에 로드하고 float16을 사용\nmodel = LlamaForSequenceClassification.from_pretrained(\n    model_name_or_path,\n    torch_dtype=torch.float16,\n    device_map=device_map,\n)\n\n# 회전 임베딩 준비\nconfig = model.config\ndim = config.hidden_size // config.num_attention_heads\n\n# RoPE(회전 위치 인코딩)에 대한 역주파수 계산\ninv_freq = 1.0 / (\n    config.rope_theta ** (torch.arange(0, dim, 2, dtype=torch.float32) / dim)\n)\ninv_freq0 = inv_freq.to(\"cuda:0\")\ninv_freq1 = inv_freq.to(\"cuda:1\")\n\n# 파이프라인 실행을 이용한 추론 루프\nis_first = True\nhidden_states = None\nouts = []\n\n# 전체 배치에 대해서\nfor batch in tqdm(dataloader):\n    for micro_batch in batch:\n        # 입력 토큰을 GPU 0으로 이동\n        input_ids = to_device(micro_batch[\"input_ids\"], \"cuda:0\")\n\n        # 시퀀스 관련 정보 준비\n        seq_info = dict(\n            cu_seqlens=micro_batch[\"cu_seqlens\"],\n            position_ids=micro_batch[\"position_ids\"],\n            max_seq_len=micro_batch[\"max_seq_len\"],\n            attn_bias=BlockDiagonalCausalMask.from_seqlens(micro_batch[\"seq_lens\"]),\n        )\n        seq_info = to_device(seq_info, \"cuda:0\")\n\n        # 첫 실행에서는 파트 1만 실행하고 상태 저장\n        if is_first:\n            with torch.no_grad(), torch.cuda.amp.autocast():\n                prev_hidden_states = model.forward_part1(input_ids, seq_info, inv_freq0)\n            is_first = False\n            # 다음 단계 위해 중간 출력을 GPU 1로 이동\n            prev_seq_info, prev_hidden_states = to_device(\n                [seq_info, prev_hidden_states], \"cuda:1\"\n            )\n            continue\n\n        # 이전 마이크로배치에 대해 2부 실행하고 현재에 대해 1부 실행\n        with torch.no_grad(), torch.cuda.amp.autocast():\n            # 이전 숨겨진 상태에 대한 최종 logit값 계산\n            logits = model.forward_part2(prev_hidden_states, prev_seq_info, inv_freq1)\n            hidden_states = model.forward_part1(input_ids, seq_info, inv_freq0)\n            # 다음 마이크로배치에 대한 숨겨진 상태를 계산\n            prev_seq_info, prev_hidden_states = to_device(\n                [seq_info, hidden_states], \"cuda:1\"\n            )\n            outs.append(logits.cpu())\n\n# --- Process final micro-batch (no part1 needed) ---\nwith torch.no_grad(), torch.cuda.amp.autocast():\n    logits = model.forward_part2(prev_hidden_states, prev_seq_info, inv_freq1)\n    outs.append(logits.cpu())\n\n# 모든 logit을 연결하고 확률 계산\npred = torch.cat(outs, dim=0)        \nprob = pred.softmax(-1)              \nprint(dataset.evaluate(prob.numpy()))\n\n# 예측 확률을 파일에 저장\nnp.save('prob_m3.npy', prob)\nprint(\"prob_m3 saved\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T12:42:13.914139Z","iopub.execute_input":"2025-11-01T12:42:13.914468Z","iopub.status.idle":"2025-11-01T12:42:13.922284Z","shell.execute_reply.started":"2025-11-01T12:42:13.914420Z","shell.execute_reply":"2025-11-01T12:42:13.921465Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python llama.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T12:42:13.923661Z","iopub.execute_input":"2025-11-01T12:42:13.924126Z","execution_failed":"2025-11-01T12:43:51.596Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n\nprob = np.load('prob_m3.npy')\n\nprint(prob[:5])\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-11-01T12:43:51.596Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Inference: Faiss","metadata":{}},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom transformers import AutoModel\nfrom transformers import AutoTokenizer\nfrom transformers import AutoTokenizer\nfrom sklearn.preprocessing import MinMaxScaler\nimport faiss\n\nmodel_load_path = '/kaggle/input/some-pack/sentence-transformer-model' \nsentence_model = SentenceTransformer(model_load_path)\n\ntest_data = pd.read_csv('/kaggle/input/llm-classification-finetuning/test.csv')\n\nclass CustomDebertaModel(nn.Module):\n    def __init__(self, model_name, num_labels, feature_dim=2, dropout_rate=0.1):\n        super(CustomDebertaModel, self).__init__()\n        \n        # DeBERTa 모델 초기화\n        self.base_model = AutoModel.from_pretrained(model_name)\n        \n        # 유사성 특징을 위한 특징 타워 (소규모 MLP)\n        self.feature_fc = nn.Sequential(\n            # 입력 유사성 특징을 128차원 공간에 매핑\n            nn.Linear(feature_dim, 128),\n            nn.ReLU(),\n            nn.Dropout(p=dropout_rate),\n            # 텍스트 임베딩과 동일한 크기로 프로젝트 조정\n            nn.Linear(128, self.base_model.config.hidden_size),  \n            nn.ReLU()\n        )\n        \n        # 텍스트와 유사성 임베딩 간의 상호작용을 허용하는 주의 메커니즘\n        self.attention = nn.MultiheadAttention(\n            embed_dim=self.base_model.config.hidden_size,\n            # 어텐션 헤드 수\n            num_heads=4,  \n            # 배치 우선 입력 포맷 활성화\n            batch_first=True  \n        )\n        \n        # 정규화를 위한 드롭아웃 레이어\n        self.dropout = nn.Dropout(p=dropout_rate)\n\n        # 최종 분류기 레이어 (분류를 위한 MLP)\n        self.classifier = nn.Sequential(\n            nn.Linear(self.base_model.config.hidden_size * 2, self.base_model.config.hidden_size),  # Combine text + attention features\n            nn.ReLU(),\n            nn.Dropout(p=dropout_rate),\n            nn.Linear(self.base_model.config.hidden_size, num_labels)\n        )\n\n    def forward(self, input_ids, attention_mask, similarity_features, labels=None):\n        # 텍스트 타워: DeBERTa에서 [CLS] 토큰 임베딩 추출\n        base_outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n        text_embeddings = base_outputs.last_hidden_state[:, 0, :]  # [batch_size, hidden_size]\n\n        # 특징 타워: MLP를 통한 프로세스 유사성 특징 처리\n        similarity_embeds = self.feature_fc(similarity_features)  # [batch_size, hidden_size]\n\n        # 주의 메커니즘을 활용한 교차 모달 상호작용\n        query = text_embeddings.unsqueeze(1)       # 형상: [batch_size, 1, hidden_size]\n        key_value = similarity_embeds.unsqueeze(1) # 형상: [batch_size, 1, hidden_size]\n        attention_output, _ = self.attention(query, key_value, key_value)  # [batch_size, 1, hidden_size]\n\n        # 텍스트와 어텐디드 유사성 특징을 결합\n        combined_features = torch.cat([text_embeddings, attention_output.squeeze(1)], dim=1)\n\n        # 드롭아웃 및 분류 헤드를 적용\n        logits = self.classifier(self.dropout(combined_features))\n\n        # logit 값을 포함한 출력 사전\n        outputs = {\"logits\": logits}\n        \n        # 라벨이 제공되는 경우, 교차 엔트로피 손실을 계산\n        if labels is not None:\n            loss_fn = nn.CrossEntropyLoss()\n            outputs[\"loss\"] = loss_fn(logits, labels)\n\n        return outputs","metadata":{"trusted":true,"execution":{"execution_failed":"2025-11-01T12:43:51.596Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nimport faiss\n\ntest_data = pd.read_csv('/kaggle/input/llm-classification-finetuning/test.csv')\n\n# FAISS 사용해 의미적 유사도 점수 계산\ndef compute_semantic_features_with_faiss(df):\n    # 프롬프트와 응답을 목록으로 추출\n    prompts = df['prompt'].tolist()\n    responses_a = df['response_a'].tolist()\n    responses_b = df['response_b'].tolist()\n\n    # 문장 임베딩을 생성하고 정규화(단위 벡터)\n    prompt_embeddings = np.array(sentence_model.encode(prompts))\n    prompt_embeddings = prompt_embeddings / np.linalg.norm(prompt_embeddings, axis=1, keepdims=True)\n\n    response_a_embeddings = np.array(sentence_model.encode(responses_a))\n    response_a_embeddings = response_a_embeddings / np.linalg.norm(response_a_embeddings, axis=1, keepdims=True)\n\n    response_b_embeddings = np.array(sentence_model.encode(responses_b))\n    response_b_embeddings = response_b_embeddings / np.linalg.norm(response_b_embeddings, axis=1, keepdims=True)\n\n    # 임베딩 차원 결정\n    dim = prompt_embeddings.shape[1]\n    \n    # 내적(코사인 유사도) 사용하여 FAISS 인덱스 생성\n    index_flat = faiss.IndexFlatIP(dim)\n\n    # 응답 A와 프롬프트 간의 유사도 계산\n    # FAISS 인덱스에 프롬프트 임베딩 추가\n    # 상위 1개 유사도 점수 획득\n    index_flat.add(prompt_embeddings) \n    similarity_a = index_flat.search(response_a_embeddings, k=1)[0].squeeze()  \n\n    # B에 대한 유사도 재설정 및 계산\n    index_flat.reset()\n    index_flat.add(prompt_embeddings)\n    similarity_b = index_flat.search(response_b_embeddings, k=1)[0].squeeze()\n\n    # 데이터프레임에 유사도 점수 저장\n    df['similarity_a'] = similarity_a\n    df['similarity_b'] = similarity_b\n\n    return df","metadata":{"trusted":true,"execution":{"execution_failed":"2025-11-01T12:43:51.597Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_data = compute_semantic_features_with_faiss(test_data)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-11-01T12:43:51.597Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 훈련된 사용자 정의 PyTorch 모델을 로드\nmodel = torch.load(\"/kaggle/input/akemiiiiii/custom_model_dir/custom_model_complete.pth\")\n\n# 사용 가능한 경우 장치를 GPU로 설정하고 그렇지 않으면 CPU로 대체\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n# 중요: 평가 모드 설정\nmodel.eval()  \n\nprint(\"Custom model loaded successfully!\")\n\n# 훈련 중 사용된 토크나이저 로드\ntokenizer_path = \"/kaggle/input/akemiiiiii/custom_model_dir\"\ntokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n\n# 단일 테스트 샘플 전처리 함수\ndef preprocess_test_data(row):\n    # 훈련과 동일한 형식으로 입력 문자열 구성\n    input_text = f\"Prompt: {row['prompt']} Response A: {row['response_a']} Response B: {row['response_b']}\"\n\n    # 훈련과 동일한 토크나이저 설정으로 입력 토큰화\n    tokenized_inputs = tokenizer(\n        input_text,\n        truncation=True,\n        padding=\"max_length\",\n        max_length=512,\n        return_tensors=\"pt\"  # PyTorch 텐서 반환\n    )\n\n    # 유사도 점수를 추가 특징으로 추가\n    # 올바른 배치 처리를 위해 [1, feature_dim] 형식으로 구성해야 함\n    similarity = torch.tensor([[row[\"similarity_a\"], row[\"similarity_b\"]]], dtype=torch.float32)\n\n    tokenized_inputs[\"similarity_features\"] = similarity\n\n    return tokenized_inputs\n\n# 테스트 데이터셋의 모든 행에 전처리 함수 적용\nprocessed_test_data = [preprocess_test_data(row) for _, row in test_data.iterrows()]","metadata":{"trusted":true,"execution":{"execution_failed":"2025-11-01T12:43:51.597Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class TestDataset(torch.utils.data.Dataset):\n    def __init__(self, data):\n        self.data = data\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        return self.data[idx]\n\ntest_dataset = TestDataset(processed_test_data)\n\ndef collate_fn_test(batch):\n    # 입력 ID와 어텐션 마스크를 배치 차원을 따라 결합\n    input_ids = torch.cat([item[\"input_ids\"] for item in batch], dim=0)\n    attention_mask = torch.cat([item[\"attention_mask\"] for item in batch], dim=0)\n    \n    # 스택 유사성 특징\n    similarity_features = torch.cat([item[\"similarity_features\"] for item in batch], dim=0)  # shape [B, 2]\n\n    return {\n        \"input_ids\": input_ids,\n        \"attention_mask\": attention_mask,\n        \"similarity_features\": similarity_features,\n    }\n\ntest_dataloader = DataLoader(test_dataset, batch_size=8, collate_fn=collate_fn_test, shuffle=False)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-11-01T12:43:51.597Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 모델을 평가모드로 설정(드롭아웃 비활성화 필수)\nmodel.eval()\n\n# 예측 결과 저장 리스트\npredictions = []\n\n# 속도 빠르게 하기 위한 경사 계산 비활성화\nwith torch.no_grad():\n    for batch in test_dataloader:\n        # 배치 데이터를 올바른 장치(GPU/CPU)로 이동\n        input_ids = batch[\"input_ids\"].to(device)\n        attention_mask = batch[\"attention_mask\"].to(device)\n        similarity_features = batch[\"similarity_features\"].to(device)\n\n        # 전방 통과 실행(모델 추론)\n        outputs = model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            similarity_features=similarity_features\n        )\n\n        # 출력에서 원시 logit 값 추출\n        logits = outputs[\"logits\"]\n\n        # 소프트맥스로 logit을 확률로 변환\n        probs = torch.nn.functional.softmax(logits, dim=-1)\n\n        # CPU로 이동하고 나중에 사용하기 위해 NumPy로 변환\n        predictions.append(probs.cpu().numpy())\n\n# 모든 배치 결과를 하나의 NumPy 배열로 결합클릭하여 적용\npredictions = np.concatenate(predictions, axis=0)\n\n# 최종 예측 출력\nprint(predictions)\n\n# 예측을 .npy에 저장\nnp.save('prob_faiss.npy', predictions)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-11-01T12:43:51.597Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Create submission file","metadata":{}},{"cell_type":"code","source":"df = pd.read_parquet(\"test.parquet\")\n\n\nprob_m0 = np.load(\"prob_m0.npy\")  # Gemma2\nprob_m3 = np.load(\"prob_m3.npy\")[:, [1, 0, 2]]  # Llama3\nprob_faiss = np.load(\"prob_faiss.npy\")  # faiss\n\n# 예측값과 가중치를 결합\n# 최적의 성능을 위해 가중치 조정\npreds = np.average(\n    [\n        prob_m0,       # Gemma2 결과\n        prob_m3,       # Llama3 결과\n        prob_faiss     # faiss 결과\n    ],\n    axis=0,\n    weights=[0.5, 0.5, 0.0]  # 각 모델 가중치\n)\n\n# 제출용\nsub = pd.DataFrame({\n    \"id\": df[\"id\"],\n    \"winner_model_a\": preds[:, 0],\n    \"winner_model_b\": preds[:, 1],\n    \"winner_tie\": preds[:, 2],\n})\n\n# Save to CSV\nsub.to_csv(\"submission.csv\", index=False)\nprint(sub.head())\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-11-01T12:43:51.597Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Reference\n- LMSYS - Chatbot Arena Human Preference Predictions 2nd place solution - https://www.kaggle.com/competitions/lmsys-chatbot-arena/discussion/527685\n- Blue - https://www.kaggle.com/code/blue0924/finetuning-test2","metadata":{}}]}